@article{key_name,
    author  = {Peter Adams},
    title   = {The title of the work},
    journal = {The name of the journal},
    year    = {1993},
    number  = {2},
    pages   = {201-213},
    month   = {7},
    note    = {An optional note},
    volume  = {4}
}
@InProceedings{LDES,
author="Van Lancker, Dwight
and Colpaert, Pieter
and Delva, Harm
and Van de Vyvere, Brecht
and Mel{\'e}ndez, Juli{\'a}n Rojas
and Dedecker, Ruben
and Michiels, Philippe
and Buyle, Raf
and De Craene, Annelies
and Verborgh, Ruben",
editor="Brambilla, Marco
and Chbeir, Richard
and Frasincar, Flavius
and Manolescu, Ioana",
title="Publishing Base Registries as Linked Data Event Streams",
booktitle="Web Engineering",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="28--36",
abstract="Fostering interoperability, Public Sector Bodies (PSBs) maintain datasets that should become queryable as an integrated Knowledge Graph (KG). While some PSBs allow to query a part of the KG on their servers, others favor publishing data dumps allowing the querying to happen on third party servers. As the budget of a PSB to publish their dataset on the Web is finite, PSBs need guidance on what interface to offer first. A core API can be designed that covers the core tasks of Base Registries, which is a well-defined term in Flanders for the management of authoritative datasets. This core API should be the basis on which an ecosystem of data services can be built. In this paper, we introduce the concept of a Linked Data Event Stream (LDES) for datasets like air quality sensors and observations or a registry of officially registered addresses. We show that extra ecosystem requirements can be built on top of the LDES using a generic fragmenter. By using hypermedia for describing the LDES as well as the derived datasets, agents can dynamically discover their best way through the KG, and server administrators can dynamically add or remove functionality based on costs and needs. This way, we allow PSBs to prioritize API functionality based on three tiers: (i) the LDES, (ii) intermediary indexes and (iii) querying interfaces. While the ecosystem will never be feature-complete, based on the market needs, PSBs as well as market players can fill in gaps as requirements evolve.",
isbn="978-3-030-74296-6"
}

@InProceedings{Substring,
author="Van de Vyvere, Brecht
and D'Huynslager, Olivier Van
and Atauil, Achraf
and Segers, Maarten
and Van Campe, Leen
and Vandekeybus, Niels
and Teugels, Sofie
and Saenko, Alina
and Pauwels, Pieter-Jan
and Colpaert, Pieter",
editor="Garoufallou, Emmanouel
and Ovalle-Perandones, Mar{\'i}a-Antonia
and Vlachidis, Andreas",
title="Publishing Cultural Heritage Collections of Ghent with Linked Data Event Streams",
booktitle="Metadata and Semantic Research",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="357--369",
abstract="Cultural heritage institutions maintain digital artefacts of their collections using Collection Management Software (CMS). In order to attract new audiences, these data should be interoperable with and reusable within other Web APIs. In this article, we explain how we applied Flemish Linked Data Standards (OSLO) to make the data within the Axiell Collections CMS interoperable, and how we applied the method of Linked Data Event Streams (LDES) for making the data reusable. The LDES has been successfully adopted by third parties to then host subject pages, a SPARQL endpoint, a substring fragmentation for autocompletion purposes, and a IIIF enriched LDES. To this end, we see LDES as the core Web API of a CMS, allowing third parties to take up other querying and processing tasks on their own machines.",
isbn="978-3-030-98876-0"
}

@Inbook{GeoWeb,
author="Jones, Jim
and Kuhn, Werner
and Ke{\ss}ler, Carsten
and Scheider, Simon",
editor="Huerta, Joaqu{\'i}n
and Schade, Sven
and Granell, Carlos",
title="Making the Web of Data Available Via Web Feature Services",
bookTitle="Connecting a Digital Europe Through Location and Place",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="341--361",
abstract="Interoperability is the main challenge on the way to efficiently find and access spatial data on the web. Significant contributions regarding interoperability have been made by the Open Geospatial Consortium (OGC), where web service standards to publish and download spatial data have been established. The OGCs GeoSPARQL specification targets spatial data on the Web as Linked Open Data (LOD) by providing a comprehensive vocabulary for annotation and querying. While OGC web service standards are widely implemented in Geographic Information Systems (GIS) and offer a seamless service infrastructure, the LOD approach offers structured techniques to interlink and semantically describe spatial information. It is currently not possible to use LOD as a data source for OGC web services. In this chapter we make a suggestion for technically linking OGC web services and LOD as a data source, and we explore and discuss its benefits. We describe and test an adapter that enables access to geographic LOD datasets from within OGC Web Feature Service (WFS), enabling most current GIS to access the Web of Data. We discuss performance tests by comparing the proposed adapter to a reference WFS implementation.",
isbn="978-3-319-03611-3",
doi="10.1007/978-3-319-03611-3_20",
url="https://doi.org/10.1007/978-3-319-03611-3_20"
}
@Article{DCTerms,
author={Baker, Thomas},
title={Libraries, languages of description, and linked data: a Dublin Core perspective},
journal={Library Hi Tech},
year={2012},
month={Jan},
day={01},
publisher={Emerald Group Publishing Limited},
volume={30},
number={1},
pages={116-133},
abstract={Purpose Library‐world ``languages of description'' are increasingly being expressed using the resource description framework (RDF) for compatibility with linked data approaches. This article aims to look at how issues around the Dublin Core, a small ``metadata element set,'' exemplify issues that must be resolved in order to ensure that library data meet traditional standards for quality and consistency while remaining broadly interoperable with other data sources in the linked data environment. Design/methodology/approach The article focuses on how the Dublin Core -- originally seen, in traditional terms, as a simple record format -- came increasingly to be seen as an RDF vocabulary for use in metadata based on a ``statement'' model, and how new approaches to metadata evolved to bridge the gap between these models. Findings The translation of library standards into RDF involves the separation of languages of description, per se, from the specific data formats into which they have for so long been embedded. When defined with ``minimal ontological commitment,'' languages of description lend themselves to the sort of adaptation that is inevitably a part of any human linguistic activity. With description set profiles, the quality and consistency of data traditionally required for sharing records among libraries can be ensured by placing precise constraints on the content of data records -- without compromising the interoperability of the underlying vocabularies in the wider linked data context. Practical implications In today's environment, library data must continue to meet high standards of consistency and quality, yet it must be possible to link or merge the data with sources that follow other standards. Placing constraints on the data created, more than on the underlying vocabularies, allows both requirements to be met. Originality/value This paper examines how issues around the Dublin Core exemplify issues that must be resolved to ensure library data meet quality and consistency standards while remaining interoperable with other data sources.},
issn={0737-8831},
doi={10.1108/07378831211213256},
url={https://doi.org/10.1108/07378831211213256}
}
@TechReport{DCAT,
  author      = "Alejandra Gonzalez Beltran and Simon Cox and David Browning and Andrea Perego and Riccardo Albertoni and Peter Winstanley",
  title       = "Data Catalog Vocabulary ({DCAT}) - Version 2",
  month       = feb,
  note        = "https://www.w3.org/TR/2020/REC-vocab-dcat-2-20200204/",
  year        = "2020",
  bibsource   = "https://w2.syronex.com/jmr/w3c-biblio",
  type        = "{W3C} Recommendation",
  institution = "W3C",
}
@InProceedings{VoCaLS,
author="Tommasini, Riccardo
and Sedira, Yehia Abo
and Dell'Aglio, Daniele
and Balduini, Marco
and Ali, Muhammad Intizar
and Le Phuoc, Danh
and Della Valle, Emanuele
and Calbimonte, Jean-Paul",
editor="Vrande{\v{c}}i{\'{c}}, Denny
and Bontcheva, Kalina
and Su{\'a}rez-Figueroa, Mari Carmen
and Presutti, Valentina
and Celino, Irene
and Sabou, Marta
and Kaffee, Lucie-Aim{\'e}e
and Simperl, Elena",
title="VoCaLS: Vocabulary and Catalog of Linked Streams",
booktitle="The Semantic Web -- ISWC 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="256--272",
abstract="The nature of Web data is changing. The popularity of news feeds and social media, the rise of the Web of Things, and the adoption of sensor technologies are examples of streaming data that reached the Web scale. The different nature of streaming data calls for specific solutions to problems like data integration and analytics. There is a need for streaming-specific Web resources: new vocabularies to describe, find and select streaming data sources, and systems that can cooperate dynamically to solve stream processing tasks. To foster interoperability between these streaming services on the Web, we propose the Vocabulary {\&} Catalog of Linked Streams (VoCaLS). VoCaLS is a three-module ontology to (i) publish streaming data following Linked Data principles, (ii) describe streaming services and (iii) track the provenance of stream processing.",
isbn="978-3-030-00668-6"
}
@inproceedings{VOID,
  title={Describing linked datasets},
  author={Alexander, Keith and Cyganiak, Richard and Hausenblas, Michael and Zhao, Jun},
  booktitle={LDOW},
  year={2009}
}

@Article{QueryDiscovery1,
author={Ben Ellefi, Mohamed
and Bellahsene, Zohra
and Breslin, John G.
and Demidova, Elena
and Dietze, Stefan
and Szyma{\'{n}}ski, Julian
and Todorov, Konstantin},
title={RDF dataset profiling -- a survey of features, methods, vocabularies and applications},
journal={Semantic Web},
year={2018},
publisher={IOS Press},
volume={9},
pages={677-705},
keywords={Linked Data assessment; RDF dataset profiling; dataset features; dataset profiling vocabularies},
abstract={The Web of Data, and in particular Linked Data, has seen tremendous growth over the past years. However, reuse and take-up of these rich data sources is often limited and focused on a few well-known and established RDF datasets. This can be partially attributed to the lack of reliable and up-to-date information about the characteristics of available datasets. While RDF datasets vary heavily with respect to the features related to quality, provenance, interlinking, licenses, statistics and dynamics, reliable information about such features is essential to enable dataset discovery and selection in tasks such as entity linking, distributed query, search or question answering. Even though there exists a wealth of works contributing to the task of dataset profiling in general, these works are spread across a wide range of communities. In this survey, we provide a first comprehensive overview of the RDF dataset profiling features, methods, tools and vocabularies. We organize these building blocks of dataset profiling in a taxonomy and illustrate the links between the dataset profiling and feature extraction approaches and several application domains. This survey is aimed towards data practitioners, data providers and scientists, spanning a large range of communities and drawing from different fields such as dataset profiling, assessment, summarization and characterization. Ultimately, this work is intended to facilitate the reader to identify the relevant features for building a dataset profile for intended applications together with the methods and tools capable of extracting these features from the datasets as well as vocabularies to describe the extracted features and make them available.},
note={5},
issn={2210-4968},
doi={10.3233/SW-180294},
url={https://doi.org/10.3233/SW-180294}
}
@inproceedings{QueryDiscovery2,
author = {Michel, Franck and Faron-Zucker, Catherine and Corby, Olivier and Gandon, Fabien},
title = {Enabling Automatic Discovery and Querying of Web APIs at Web Scale Using Linked Data Standards},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317073},
doi = {10.1145/3308560.3317073},
abstract = {To help in making sense of the ever-increasing number of data sources available on the Web, in this article we tackle the problem of enabling automatic discovery and querying of data sources at Web scale. To pursue this goal, we suggest to (1) provision rich descriptions of data sources and query services thereof, (2) leverage the power of Web search engines to discover data sources, and (3) rely on simple, well-adopted standards that come with extensive tooling. We apply these principles to the concrete case of SPARQL micro-services that aim at querying Web APIs using SPARQL. The proposed solution leverages SPARQL Service Description, SHACL, DCAT, VoID, Schema.org and Hydra to express a rich functional description that allows a software agent to decide whether a micro-service can help in carrying out a certain task. This description can be dynamically transformed into a Web page embedding rich markup data. This Web page is both a human-friendly documentation and a machine-readable description that makes it possible for humans and machines alike to discover and invoke SPARQL micro-services at Web scale, as if they were just another data source. We report on a prototype implementation that is available on-line for test purposes, and that can be effectively discovered using Google’s Dataset Search engine.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {883–892},
numpages = {10},
keywords = {Web API, Web service, SPARQL, Linked Data, discovery, dataset},
location = {San Francisco, USA},
series = {WWW '19}
}
@TechReport{PPLAN,
  author      = "Danial Garijo and Yolando Gil",
  title       = "The {P-Plan} Ontology",
  month       = mar,
  note        = "http://vocab.linkeddata.es/p-plan/index.html",
  year        = "2014",
}
@TechReport{PROVO,
  author      = "Timothy Lebo and Satya Sahoo and Deborah McGuinness",
  title       = "{PROV}-O: The {PROV} Ontology",
  month       = apr,
  note        = "https://www.w3.org/TR/2013/REC-prov-o-20130430/",
  year        = "2013",
  bibsource   = "https://w2.syronex.com/jmr/w3c-biblio",
  type        = "{W3C} Recommendation",
  institution = "W3C",
}
@InProceedings{SPARQL,
author="Buil-Aranda, Carlos
and Hogan, Aidan
and Umbrich, J{\"u}rgen
and Vandenbussche, Pierre-Yves",
editor="Alani, Harith
and Kagal, Lalana
and Fokoue, Achille
and Groth, Paul
and Biemann, Chris
and Parreira, Josiane Xavier
and Aroyo, Lora
and Noy, Natasha
and Welty, Chris
and Janowicz, Krzysztof",
title="SPARQL Web-Querying Infrastructure: Ready for Action?",
booktitle="The Semantic Web -- ISWC 2013",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="277--293",
abstract="Hundreds of public SPARQL endpoints have been deployed on the Web, forming a novel decentralised infrastructure for querying billions of structured facts from a variety of sources on a plethora of topics. But is this infrastructure mature enough to support applications? For 427 public SPARQL endpoints registered on the DataHub, we conduct various experiments to test their maturity. Regarding discoverability, we find that only one-third of endpoints make descriptive meta-data available, making it difficult to locate or learn about their content and capabilities. Regarding interoperability, we find patchy support for established SPARQL features like ORDER BY as well as (understandably) for new SPARQL 1.1 features. Regarding efficiency, we show that the performance of endpoints for generic queries can vary by up to 3--4 orders of magnitude. Regarding availability, based on a 27-month long monitoring experiment, we show that only 32.2{\%} of public endpoints can be expected to have (monthly) ``two-nines'' uptimes of 99--100{\%}.",
isbn="978-3-642-41338-4"
}
@online{Nautirust,
  author = {Arthur Vercruysse and Sitt Min Oo},
  title = {{Nautirust} Connector architecture orchestrator },
  year = 2022,
  url = {https://github.com/ajuvercr/nautirust},
  urldate = {2022-08-24}
}
